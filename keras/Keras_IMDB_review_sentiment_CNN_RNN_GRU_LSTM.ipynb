{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB sentiment classification task\n",
    "\n",
    "IMDB provided a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. \n",
    "\n",
    "We will train classifiers using **CNN, RNN, LSTM and GRU** for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "np.random.seed(7) # fix random seed for reproducibility\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Keras Import\n",
    "\n",
    "from keras.datasets import imdb\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, TimeDistributed, RepeatVector\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, Conv1D, MaxPooling1D\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import image,sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM, GRU, SimpleRNN\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint#### IMDB sentiment classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation - IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(x_train, y_train), (x_test, y_test)** = imdb.load_data(path=\"imdb_full.pkl\",\n",
    "                                                      num_words=None,\n",
    "                                                      skip_top=0,\n",
    "                                                      maxlen=None,\n",
    "                                                      seed=113,\n",
    "                                                      start_char=1)\n",
    "\n",
    "#### Arguments:\n",
    "**Path**: if you do not have the data locally (at '~/.keras/datasets/' + path), it will be downloaded to this location.\n",
    "\n",
    "**num_words**: integer or None. Top most frequent words to consider. Any less frequent word will appear as 0 in the sequence data.\n",
    "\n",
    "**skip_top**: integer. Top most frequent words to ignore (they will appear as 0s in the sequence data).\n",
    "\n",
    "**maxlen**: int. Maximum sequence length. Any longer sequence will be truncated.\n",
    "\n",
    "**seed**: int. Seed for reproducible data shuffling.\n",
    "\n",
    "**start_char**: char. The start of a sequence will be marked with this character. Set to 1 because 0 is usually the padding character.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: \n",
      "(50000,)\n",
      "(50000,)\n",
      "Classes: \n",
      "[0 1]\n",
      "Number of words: \n",
      "88585\n"
     ]
    }
   ],
   "source": [
    "# summarize size\n",
    "print(\"Training data: \")\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(\"Classes: \")\n",
    "print(np.unique(y))\n",
    "print(\"Number of words: \")\n",
    "print(len(np.unique(np.hstack(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length: \n",
      "Mean 234.76 words (172.911495)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIEkl9f8B9tHQ1XnvY0YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1afc1883d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarize review length\n",
    "print(\"Review length: \")\n",
    "result = [len(x) for x in X]\n",
    "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "# plot review length\n",
    "plt.boxplot(result)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad sequence\n",
    "keras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.)\n",
    "\n",
    "#### Arguments:\n",
    "\n",
    "**sequences**: List of lists of int or float.\n",
    "maxlen: None or int. Maximum sequence length, longer sequences are truncated and shorter sequences are padded with zeros at the end.\n",
    "\n",
    "**dtype**: datatype of the Numpy array returned.\n",
    "\n",
    "**padding**: 'pre' or 'post', pad either before or after each sequence.\n",
    "\n",
    "**truncating**: 'pre' or 'post', remove values from sequences larger than maxlen either in the beginning or in the end of the sequence\n",
    "\n",
    "**value**: float, value to pad the sequences to the desired value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example no.: 5\n",
      "\n",
      "Length of example: 147\n",
      "[1, 249, 1323, 7, 61, 113, 10, 10, 13, 1637, 14, 20, 56, 33, 2401, 18, 457, 88, 13, 2626, 1400, 45, 3171, 13, 70, 79, 49, 706, 919, 13, 16, 355, 340, 355, 1696, 96, 143, 4, 22, 32, 289, 7, 61, 369, 71, 2359, 5, 13, 16, 131, 2073, 249, 114, 249, 229, 249, 20, 13, 28, 126, 110, 13, 473, 8, 569, 61, 419, 56, 429, 6, 1513, 18, 35, 534, 95, 474, 570, 5, 25, 124, 138, 88, 12, 421, 1543, 52, 725, 2, 61, 419, 11, 13, 1571, 15, 1543, 20, 11, 4, 2, 5, 296, 12, 3524, 5, 15, 421, 128, 74, 233, 334, 207, 126, 224, 12, 562, 298, 2167, 1272, 7, 2601, 5, 516, 988, 43, 8, 79, 120, 15, 595, 13, 784, 25, 3171, 18, 165, 170, 143, 19, 14, 5, 2, 6, 226, 251, 7, 61, 113]\n",
      "\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (25000, 500)\n",
      "X_test shape: (25000, 500)\n",
      "\n",
      "Length of example after pad_sequences: 500\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    1  249 1323    7   61  113   10\n",
      "   10   13 1637   14   20   56   33 2401   18  457   88   13 2626 1400   45\n",
      " 3171   13   70   79   49  706  919   13   16  355  340  355 1696   96  143\n",
      "    4   22   32  289    7   61  369   71 2359    5   13   16  131 2073  249\n",
      "  114  249  229  249   20   13   28  126  110   13  473    8  569   61  419\n",
      "   56  429    6 1513   18   35  534   95  474  570    5   25  124  138   88\n",
      "   12  421 1543   52  725    2   61  419   11   13 1571   15 1543   20   11\n",
      "    4    2    5  296   12 3524    5   15  421  128   74  233  334  207  126\n",
      "  224   12  562  298 2167 1272    7 2601    5  516  988   43    8   79  120\n",
      "   15  595   13  784   25 3171   18  165  170  143   19   14    5    2    6\n",
      "  226  251    7   61  113]\n"
     ]
    }
   ],
   "source": [
    "top_words = 5000\n",
    "maxlen = 500  # cut texts after this number of words (among top top_words most common words)\n",
    "batch_size = 32\n",
    "max_features = 100\n",
    "batch_size = 32\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n",
    "example = 5\n",
    "print('\\nExample no.:',example)\n",
    "print('\\nLength of example:',len(X_train[example-1]))\n",
    "print(X_train[example-1])\n",
    "\n",
    "print(\"\\nPadding sequences\")\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen, padding='pre', truncating='post', value=0.)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen, padding='pre', truncating='post', value=0.)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('\\nLength of example after pad_sequences:',len(X_train[example-1]))\n",
    "print(X_train[example-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding building \n",
    "\n",
    "keras.layers.embeddings.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, \n",
    "                                  activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)\n",
    "\n",
    "#### Arguments:\n",
    "\n",
    "**input_dim**: int > 0. Size of the vocabulary, ie. 1 + maximum integer index occurring in the input data.\n",
    "    \n",
    "**output_dim**: int >= 0. Dimension of the dense embedding.\n",
    "    \n",
    "**embeddings_initializer**: Initializer for the embeddings matrix (see initializers).\n",
    "    \n",
    "**embeddings_regularizer**: Regularizer function applied to the embeddings matrix (see regularizer).\n",
    "    \n",
    "**embeddings_constraint**: Constraint function applied to the embeddings matrix (see constraints).\n",
    "    \n",
    "**mask_zero**: Whether or not the input value 0 is a special \"padding\" value that should be masked out. This is useful when\n",
    "using recurrent layers which may take variable length input. If this is True then all subsequent layers in the model need to support masking or an exception will be raised. \n",
    "If mask_zero is set to True, as a consequence, index 0 cannot be used in the vocabulary (input_dim should equal |vocabulary| + 2).\n",
    "\n",
    "**input_length**: Length of input sequences, when it is constant. This argument is required if you are going to connect  Flatten then Dense layers upstream (without it, the shape of the dense outputs cannot be computed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 500, 100)          500000    \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 50000)             0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 250)               12500250  \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 13,000,501.0\n",
      "Trainable params: 13,000,501.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 219s - loss: 0.4704 - acc: 0.7360 - val_loss: 0.3535 - val_acc: 0.8442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1afc408e400>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating NN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, max_features , input_length=maxlen)) #Embedding(input_dim, output_dim, input_length)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 500, 100)          500000    \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 500, 32)           9632      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 250)               2000250   \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 2,510,133.0\n",
      "Trainable params: 2,510,133.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 44s - loss: 0.4134 - acc: 0.7879 - val_loss: 0.2871 - val_acc: 0.8794\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 43s - loss: 0.2237 - acc: 0.9111 - val_loss: 0.2759 - val_acc: 0.8852\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 45s - loss: 0.1663 - acc: 0.9357 - val_loss: 0.3072 - val_acc: 0.8779\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1af92769240>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying CNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, max_features, input_length=maxlen))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent Neural networks\n",
    "====="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"https://github.com/bhavsarpratik/Deep_Learning_Notebooks/raw/master/data/images/NH_WordTimeStep_SeparateRNNs.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "keras.layers.recurrent.SimpleRNN(units, activation='tanh', use_bias=True, \n",
    "                                 kernel_initializer='glorot_uniform', \n",
    "                                 recurrent_initializer='orthogonal', \n",
    "                                 bias_initializer='zeros', \n",
    "                                 kernel_regularizer=None, \n",
    "                                 recurrent_regularizer=None, \n",
    "                                 bias_regularizer=None, \n",
    "                                 activity_regularizer=None, \n",
    "                                 kernel_constraint=None, recurrent_constraint=None, \n",
    "                                 bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arguments:\n",
    "\n",
    "<ul>\n",
    "<li><strong>units</strong>: Positive integer, dimensionality of the output space.</li>\n",
    "<li><strong>activation</strong>: Activation function to use\n",
    "    (see <a href=\"http://keras.io/activations/\">activations</a>).\n",
    "    If you pass None, no activation is applied\n",
    "    (ie. \"linear\" activation: <code>a(x) = x</code>).</li>\n",
    "<li><strong>use_bias</strong>: Boolean, whether the layer uses a bias vector.</li>\n",
    "<li><strong>kernel_initializer</strong>: Initializer for the <code>kernel</code> weights matrix,\n",
    "    used for the linear transformation of the inputs.\n",
    "    (see <a href=\"https://keras.io/initializers/\">initializers</a>).</li>\n",
    "<li><strong>recurrent_initializer</strong>: Initializer for the <code>recurrent_kernel</code>\n",
    "    weights matrix,\n",
    "    used for the linear transformation of the recurrent state.\n",
    "    (see <a href=\"https://keras.io/initializers/\">initializers</a>).</li>\n",
    "<li><strong>bias_initializer</strong>: Initializer for the bias vector\n",
    "    (see <a href=\"https://keras.io/initializers/\">initializers</a>).</li>\n",
    "<li><strong>kernel_regularizer</strong>: Regularizer function applied to\n",
    "    the <code>kernel</code> weights matrix\n",
    "    (see <a href=\"https://keras.io/regularizers/\">regularizer</a>).</li>\n",
    "<li><strong>recurrent_regularizer</strong>: Regularizer function applied to\n",
    "    the <code>recurrent_kernel</code> weights matrix\n",
    "    (see <a href=\"https://keras.io/regularizers/\">regularizer</a>).</li>\n",
    "<li><strong>bias_regularizer</strong>: Regularizer function applied to the bias vector\n",
    "    (see <a href=\"https://keras.io/regularizers/\">regularizer</a>).</li>\n",
    "<li><strong>activity_regularizer</strong>: Regularizer function applied to\n",
    "    the output of the layer (its \"activation\").\n",
    "    (see <a href=\"https://keras.io/regularizers/\">regularizer</a>).</li>\n",
    "<li><strong>kernel_constraint</strong>: Constraint function applied to\n",
    "    the <code>kernel</code> weights matrix\n",
    "    (see <a href=\"https://keras.io/constraints/\">constraints</a>).</li>\n",
    "<li><strong>recurrent_constraint</strong>: Constraint function applied to\n",
    "    the <code>recurrent_kernel</code> weights matrix\n",
    "    (see <a href=\"https://keras.io/constraints/\">constraints</a>).</li>\n",
    "<li><strong>bias_constraint</strong>: Constraint function applied to the bias vector\n",
    "    (see <a href=\"https://keras.io/constraints/\">constraints</a>).</li>\n",
    "<li><strong>dropout</strong>: Float between 0 and 1.\n",
    "    Fraction of the units to drop for\n",
    "    the linear transformation of the inputs.</li>\n",
    "<li><strong>recurrent_dropout</strong>: Float between 0 and 1.\n",
    "    Fraction of the units to drop for\n",
    "    the linear transformation of the recurrent state.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backprop Through time  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contrary to feed-forward neural networks, the RNN is characterized by the ability of encoding longer past information, thus very suitable for sequential models. The BPTT extends the ordinary BP algorithm to suit the recurrent neural\n",
    "architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<img src =\"https://github.com/bhavsarpratik/Deep_Learning_Notebooks/raw/master/data/images/rnn-bptt-with-gradients.png\" width=\"55%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference**: [Backpropagation through Time](http://ir.hit.edu.cn/~jguo/docs/notes/bptt.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 290s - loss: 0.7224 - acc: 0.5278 - val_loss: 0.6269 - val_acc: 0.6404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1afb88beb00>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying RNN model\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim =top_words, output_dim = max_features, input_length=maxlen))\n",
    "model.add(SimpleRNN(128))  \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(\"Train...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A LSTM network is an artificial neural network that contains LSTM blocks instead of, or in addition to, regular network units. A LSTM block may be described as a \"smart\" network unit that can remember a value for an arbitrary length of time. \n",
    "\n",
    "Unlike traditional RNNs, an Long short-term memory network is well-suited to learn from experience to classify, process and predict time series when there are very long time lags of unknown size between important events. <br>\n",
    " <br>\n",
    "You can get a good understanding of them from this article <br>\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/ <br>\n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/ <br>\n",
    "http://r2rt.com/written-memories-understanding-deriving-and-extending-the-lstm.html <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<img src =\"https://github.com/bhavsarpratik/Deep_Learning_Notebooks/raw/master/data/images/LSTM3-chain.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "keras.layers.recurrent.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, \n",
    "                            kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
    "                            bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, \n",
    "                            recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n",
    "                            kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, \n",
    "                            dropout=0.0, recurrent_dropout=0.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arguments\n",
    "\n",
    "<ul>\n",
    "<li><strong>units</strong>: Positive integer, dimensionality of the output space.</li>\n",
    "<li><strong>activation</strong>: Activation function to use\n",
    "    If you pass None, no activation is applied\n",
    "    (ie. \"linear\" activation: <code>a(x) = x</code>).</li>\n",
    "<li><strong>recurrent_activation</strong>: Activation function to use\n",
    "    for the recurrent step.</li>\n",
    "<li><strong>use_bias</strong>: Boolean, whether the layer uses a bias vector.</li>\n",
    "<li><strong>kernel_initializer</strong>: Initializer for the <code>kernel</code> weights matrix,\n",
    "    used for the linear transformation of the inputs.</li>\n",
    "<li><strong>recurrent_initializer</strong>: Initializer for the <code>recurrent_kernel</code>\n",
    "    weights matrix,\n",
    "    used for the linear transformation of the recurrent state.</li>\n",
    "<li><strong>bias_initializer</strong>: Initializer for the bias vector.</li>\n",
    "<li><strong>unit_forget_bias</strong>: Boolean.\n",
    "    If True, add 1 to the bias of the forget gate at initialization.\n",
    "    Setting it to true will also force <code>bias_initializer=\"zeros\"</code>.\n",
    "    This is recommended in <a href=\"http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf\">Jozefowicz et al.</a></li>\n",
    "<li><strong>kernel_regularizer</strong>: Regularizer function applied to\n",
    "    the <code>kernel</code> weights matrix.</li>\n",
    "<li><strong>recurrent_regularizer</strong>: Regularizer function applied to\n",
    "    the <code>recurrent_kernel</code> weights matrix.</li>\n",
    "<li><strong>bias_regularizer</strong>: Regularizer function applied to the bias vector.</li>\n",
    "<li><strong>activity_regularizer</strong>: Regularizer function applied to\n",
    "    the output of the layer (its \"activation\").</li>\n",
    "<li><strong>kernel_constraint</strong>: Constraint function applied to\n",
    "    the <code>kernel</code> weights matrix.</li>\n",
    "<li><strong>recurrent_constraint</strong>: Constraint function applied to\n",
    "    the <code>recurrent_kernel</code> weights matrix.</li>\n",
    "<li><strong>bias_constraint</strong>: Constraint function applied to the bias vector.</li>\n",
    "<li><strong>dropout</strong>: Float between 0 and 1.\n",
    "    Fraction of the units to drop for\n",
    "    the linear transformation of the inputs.</li>\n",
    "<li><strong>recurrent_dropout</strong>: Float between 0 and 1.\n",
    "    Fraction of the units to drop for\n",
    "    the linear transformation of the recurrent state.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gated recurrent units are a gating mechanism in recurrent neural networks. \n",
    "\n",
    "Much similar to the LSTMs, they have fewer parameters than LSTM, as they lack an output gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "keras.layers.recurrent.GRU(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, \n",
    "                           kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \n",
    "                           bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, \n",
    "                           bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, \n",
    "                           recurrent_constraint=None, bias_constraint=None, \n",
    "                           dropout=0.0, recurrent_dropout=0.0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try LSTM and GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 717s - loss: 0.4680 - acc: 0.7703 - val_loss: 0.3667 - val_acc: 0.8394\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 633s - loss: 0.3143 - acc: 0.8694 - val_loss: 0.2825 - val_acc: 0.8812\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 760s - loss: 0.2120 - acc: 0.9178 - val_loss: 0.2771 - val_acc: 0.8871\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 700s - loss: 0.1524 - acc: 0.9448 - val_loss: 0.2937 - val_acc: 0.8859\n",
      "25000/25000 [==============================] - 113s   \n",
      "Test score: 0.293660580378\n",
      "Test accuracy: 0.88588\n"
     ]
    }
   ],
   "source": [
    "# Trying GRU model\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim =top_words, output_dim = max_features, input_length=maxlen))\n",
    "model.add(GRU(128))    \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(\"Train...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=4, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trying LSTM model\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim =top_words, output_dim = max_features, input_length=maxlen))\n",
    "model.add(LSTM(128))  \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(\"Train...\")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=4, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Generation using RNN(LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read().lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))\n",
    "\n",
    "print('Vectorization...')\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "# train the model, output generated text after each iteration\n",
    "for iteration in range(1, 60):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, y, batch_size=128, nb_epoch=1)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
